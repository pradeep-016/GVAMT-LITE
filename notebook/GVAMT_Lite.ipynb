{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**GVAMT-Lite: Multimodal VAE–GRU model on MNIST with textual digit captions.**\n",
        "\n",
        "This notebook implements a practical, resource-friendly subset of the GVAMT\n",
        "architecture proposed in:\n",
        "\n",
        "    Pradeep L. et al. (2024).\n",
        "    \"Multimodal Integration in Large Language Models: Advancing AI With\n",
        "    Generative Adversarial Networks, Variational Autoencoders, and\n",
        "    Transformers (GVAMT-Model)\", IJARESM.\n",
        "\n",
        "The goal is to provide an end-to-end, executable prototype that:\n",
        "\n",
        "  * Encodes images via a VAE encoder,\n",
        "  * Encodes digit captions via a GRU-based text encoder,\n",
        "  * Fuses both modalities in a shared latent space,\n",
        "  * Reconstructs the image and generates the caption jointly.\n",
        "\n",
        "All components are intentionally lightweight so that the model can be trained\n",
        "on Google Colab with limited compute.\n"
      ],
      "metadata": {
        "id": "lMv3sOSXqEo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from typing import Tuple, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88IgbJ50kzjQ",
        "outputId": "1083f3dc-46a6-49d9-e581-3aab9d07dd6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Vocabulary and token indices\n",
        "# ---------------------------\n",
        "\n",
        "digit_words = {\n",
        "    0: \"zero\",\n",
        "    1: \"one\",\n",
        "    2: \"two\",\n",
        "    3: \"three\",\n",
        "    4: \"four\",\n",
        "    5: \"five\",\n",
        "    6: \"six\",\n",
        "    7: \"seven\",\n",
        "    8: \"eight\",\n",
        "    9: \"nine\",\n",
        "}\n",
        "\n",
        "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "word_list = special_tokens + sorted(set(digit_words.values()))\n",
        "word2idx = {w: i for i, w in enumerate(word_list)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "PAD_IDX = word2idx[\"<pad>\"]\n",
        "SOS_IDX = word2idx[\"<sos>\"]\n",
        "EOS_IDX = word2idx[\"<eos>\"]\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "print(\"Vocab size:\", vocab_size, word2idx)\n",
        "\n",
        "\n",
        "class MNISTWithText(Dataset):\n",
        "    \"\"\"MNIST digits paired with synthetic textual captions.\n",
        "\n",
        "    Each sample consists of:\n",
        "      * a grayscale image x ∈ [0, 1]^{1×28×28},\n",
        "      * a caption y in tokenized form, e.g. \"<sos> seven <eos>\".\n",
        "\n",
        "    The text is intentionally simple to isolate the multimodal fusion aspect\n",
        "    while keeping the language modeling lightweight and computationally cheap.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train: bool = True) -> None:\n",
        "        \"\"\"Initializes the multimodal MNIST–text dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train : bool, optional\n",
        "            If True, load the training split; otherwise load the test split.\n",
        "        \"\"\"\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),  # (1, 28, 28), values in [0, 1]\n",
        "        ])\n",
        "        self.mnist = datasets.MNIST(\n",
        "            root=\"./data\",\n",
        "            train=train,\n",
        "            transform=transform,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the number of available samples.\"\"\"\n",
        "        return len(self.mnist)\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_caption(label: int) -> torch.Tensor:\n",
        "        \"\"\"Encodes a digit label as a caption token sequence.\n",
        "\n",
        "        The mapping follows the global dictionary `digit_words` and wraps\n",
        "        each word with <sos> and <eos> markers:\n",
        "\n",
        "            label = 7 → \"seven\" → [<sos>, \"seven\", <eos>].\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        label : int\n",
        "            Integer digit in {0, …, 9}.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            1-D tensor of token indices with length 3 and dtype long.\n",
        "        \"\"\"\n",
        "        word = digit_words[int(label)]\n",
        "        tokens = [SOS_IDX, word2idx[word], EOS_IDX]\n",
        "        return torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"Retrieves a single multimodal example.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            Index of the requested sample.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        img : torch.Tensor\n",
        "            Tensor of shape (1, 28, 28) with values in [0, 1].\n",
        "        caption_tokens : torch.Tensor\n",
        "            1-D tensor of caption token IDs.\n",
        "        label : int\n",
        "            Integer digit label in {0, …, 9}.\n",
        "        \"\"\"\n",
        "        img, label = self.mnist[idx]\n",
        "        caption_tokens = self.encode_caption(label)\n",
        "        return img, caption_tokens, label\n",
        "\n",
        "\n",
        "def collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor, int]]):\n",
        "    \"\"\"Collates a batch of variable-length captions.\n",
        "\n",
        "    This function:\n",
        "      1. Stacks image tensors,\n",
        "      2. Pads caption sequences to the max length in the batch,\n",
        "      3. Returns caption lengths for use with packed RNNs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch : list of tuples\n",
        "        Each element is (img, caption_tokens, label) as returned by\n",
        "        `MNISTWithText.__getitem__`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    imgs : torch.Tensor\n",
        "        Tensor of shape (B, 1, 28, 28).\n",
        "    padded : torch.Tensor\n",
        "        Long tensor of shape (B, L_max) containing token IDs, padded with PAD_IDX.\n",
        "    lengths : torch.Tensor\n",
        "        1-D tensor of shape (B,) with true caption lengths.\n",
        "    labels : torch.Tensor\n",
        "        1-D tensor of shape (B,) with digit labels.\n",
        "    \"\"\"\n",
        "    imgs, labels, captions, lengths = [], [], [], []\n",
        "    for img, cap, lab in batch:\n",
        "        imgs.append(img)\n",
        "        labels.append(lab)\n",
        "        captions.append(cap)\n",
        "        lengths.append(len(cap))\n",
        "\n",
        "    max_len = max(lengths)\n",
        "    padded = torch.full((len(batch), max_len), PAD_IDX, dtype=torch.long)\n",
        "    for i, cap in enumerate(captions):\n",
        "        padded[i, :len(cap)] = cap\n",
        "\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return imgs, padded, lengths, labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7nXUknJqMda",
        "outputId": "7a6e9780-1f8b-447a-8f9c-68c86d8e8a68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 13 {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'eight': 3, 'five': 4, 'four': 5, 'nine': 6, 'one': 7, 'seven': 8, 'six': 9, 'three': 10, 'two': 11, 'zero': 12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_ds = MNISTWithText(train=True)\n",
        "test_ds = MNISTWithText(train=False)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "len(train_ds), len(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr_HgwOnqQ5J",
        "outputId": "62c32e4f-eb32-447d-aa6f-cbad963b1645"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageVAEEncoder(nn.Module):\n",
        "    \"\"\"Convolutional encoder for the image branch of GVAMT-Lite.\n",
        "\n",
        "    Maps an input image x ∈ [0, 1]^{1×28×28} to the parameters of a Gaussian\n",
        "    latent distribution N(μ, Σ), where Σ is diagonal.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim: int = 64) -> None:\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # → (B,32,14,14)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # → (B,64,7,7)\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.flat_dim = 64 * 7 * 7\n",
        "        self.fc_mu = nn.Linear(self.flat_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(self.flat_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"Encodes a batch of images into latent distribution parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor of shape (B, 1, 28, 28).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        mu : torch.Tensor\n",
        "            Mean of the approximate posterior, shape (B, latent_dim).\n",
        "        logvar : torch.Tensor\n",
        "            Log-variance of the approximate posterior, shape (B, latent_dim).\n",
        "        \"\"\"\n",
        "        h = self.conv(x)                  # (B,64,7,7)\n",
        "        h = h.view(x.size(0), -1)         # (B, flat_dim)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "\n",
        "class ImageVAEDecoder(nn.Module):\n",
        "    \"\"\"Deconvolutional decoder for reconstructing images from fused latents.\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim: int = 64) -> None:\n",
        "        super().__init__()\n",
        "        self.flat_dim = 64 * 7 * 7\n",
        "        self.fc = nn.Linear(latent_dim, self.flat_dim)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # → (B,32,14,14)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),   # → (B,1,28,28)\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Decodes a batch of latent vectors into reconstructed images.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        z : torch.Tensor\n",
        "            Latent tensor of shape (B, latent_dim).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Reconstructed images of shape (B, 1, 28, 28).\n",
        "        \"\"\"\n",
        "        h = self.fc(z)                    # (B, flat_dim)\n",
        "        h = h.view(z.size(0), 64, 7, 7)   # (B,64,7,7)\n",
        "        x_recon = self.deconv(h)          # (B,1,28,28)\n",
        "        return x_recon\n",
        "\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"GRU-based encoder for digit captions.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embed_dim: int = 32, hidden_dim: int = 64) -> None:\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, captions: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Encodes a batch of captions into fixed-size vectors.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        captions : torch.Tensor\n",
        "            Integer tensor of token IDs with shape (B, L).\n",
        "        lengths : torch.Tensor\n",
        "            1-D tensor of caption lengths, shape (B,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Sentence representations of shape (B, hidden_dim).\n",
        "        \"\"\"\n",
        "        emb = self.embed(captions)  # (B,L,E)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        _, h_n = self.gru(packed)   # h_n: (1,B,H)\n",
        "        return h_n.squeeze(0)       # (B,H)\n",
        "\n",
        "\n",
        "class TextDecoder(nn.Module):\n",
        "    \"\"\"GRU-based conditional decoder for caption generation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embed_dim: int = 32, hidden_dim: int = 64) -> None:\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, captions_in: torch.Tensor, hidden: torch.Tensor):\n",
        "        \"\"\"Performs a teacher-forced decoding pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        captions_in : torch.Tensor\n",
        "            Input token IDs (excluding final <eos>), shape (B, L_in).\n",
        "        hidden : torch.Tensor\n",
        "            Initial hidden state, typically the fused latent, shape (B, H).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        logits : torch.Tensor\n",
        "            Unnormalized token scores of shape (B, L_in, vocab_size).\n",
        "        final_hidden : torch.Tensor\n",
        "            Final GRU hidden state of shape (B, H).\n",
        "        \"\"\"\n",
        "        emb = self.embed(captions_in)            # (B,L_in,E)\n",
        "        output, h_n = self.gru(emb, hidden.unsqueeze(0))\n",
        "        logits = self.fc_out(output)             # (B,L_in,V)\n",
        "        return logits, h_n.squeeze(0)"
      ],
      "metadata": {
        "id": "0ZQlkV4JqS-a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GVAMTLite(nn.Module):\n",
        "    \"\"\"GVAMT-Lite: a lightweight multimodal generative model.\n",
        "\n",
        "    Combines:\n",
        "      * Image VAE encoder/decoder,\n",
        "      * GRU text encoder/decoder,\n",
        "      * Early fusion of image + text latents into a shared representation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        img_latent_dim: int = 64,\n",
        "        txt_hidden_dim: int = 64,\n",
        "        fused_dim: int = 64,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.img_encoder = ImageVAEEncoder(latent_dim=img_latent_dim)\n",
        "        self.img_decoder = ImageVAEDecoder(latent_dim=fused_dim)\n",
        "\n",
        "        self.txt_encoder = TextEncoder(\n",
        "            vocab_size=vocab_size, embed_dim=32, hidden_dim=txt_hidden_dim\n",
        "        )\n",
        "        self.txt_decoder = TextDecoder(\n",
        "            vocab_size=vocab_size, embed_dim=32, hidden_dim=fused_dim\n",
        "        )\n",
        "\n",
        "        self.fusion = nn.Linear(img_latent_dim + txt_hidden_dim, fused_dim)\n",
        "\n",
        "    @staticmethod\n",
        "    def reparameterize(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Applies the standard VAE reparameterization trick.\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, imgs: torch.Tensor, captions: torch.Tensor, lengths: torch.Tensor):\n",
        "        \"\"\"Runs a full forward pass of GVAMT-Lite.\n",
        "\n",
        "        Returns image reconstructions and token logits suitable for\n",
        "        computing the composite loss.\n",
        "        \"\"\"\n",
        "        # Image branch\n",
        "        mu, logvar = self.img_encoder(imgs)\n",
        "        z_img = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Text branch\n",
        "        z_txt = self.txt_encoder(captions, lengths)\n",
        "\n",
        "        # Early fusion\n",
        "        fused_input = torch.cat([z_img, z_txt], dim=1)\n",
        "        fused_z = self.fusion(fused_input)\n",
        "\n",
        "        # Image reconstruction\n",
        "        img_recon = self.img_decoder(fused_z)\n",
        "\n",
        "        # Text decoding with teacher forcing\n",
        "        captions_in = captions[:, :-1]\n",
        "        captions_tgt = captions[:, 1:]\n",
        "        logits, _ = self.txt_decoder(captions_in, fused_z)\n",
        "\n",
        "        return img_recon, mu, logvar, logits, captions_tgt\n",
        "\n",
        "\n",
        "def multimodal_loss(\n",
        "    imgs: torch.Tensor,\n",
        "    img_recon: torch.Tensor,\n",
        "    mu: torch.Tensor,\n",
        "    logvar: torch.Tensor,\n",
        "    logits: torch.Tensor,\n",
        "    captions_tgt: torch.Tensor,\n",
        "    lengths: torch.Tensor,\n",
        "    alpha_img: float = 1.0,\n",
        "    beta_kl: float = 1e-3,\n",
        "    gamma_txt: float = 1.0,\n",
        "):\n",
        "    \"\"\"Composite objective: image recon + KL + text cross-entropy.\"\"\"\n",
        "    # Image reconstruction\n",
        "    recon_loss = F.binary_cross_entropy(img_recon, imgs, reduction=\"sum\")\n",
        "\n",
        "    # KL divergence between q(z|x) and N(0, I)\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Text cross-entropy (flatten)\n",
        "    B, Lm1, V = logits.shape\n",
        "    logits_flat = logits.reshape(B * Lm1, V)\n",
        "    tgt_flat = captions_tgt.reshape(B * Lm1)\n",
        "\n",
        "    txt_loss = F.cross_entropy(\n",
        "        logits_flat, tgt_flat, ignore_index=PAD_IDX, reduction=\"sum\"\n",
        "    )\n",
        "\n",
        "    total = alpha_img * recon_loss + beta_kl * kl + gamma_txt * txt_loss\n",
        "    return total, recon_loss, kl, txt_loss"
      ],
      "metadata": {
        "id": "0KptQOXaqXq_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GVAMTLite(\n",
        "    vocab_size=vocab_size,\n",
        "    img_latent_dim=64,\n",
        "    txt_hidden_dim=64,\n",
        "    fused_dim=64,\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epochs: int = 5,\n",
        ") -> None:\n",
        "    \"\"\"Trains GVAMT-Lite for a given number of epochs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        GVAMTLite instance.\n",
        "    loader : DataLoader\n",
        "        Training dataloader.\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        Optimization algorithm.\n",
        "    device : torch.device\n",
        "        CPU or CUDA device.\n",
        "    epochs : int, optional\n",
        "        Number of full passes over the training set.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        total_loss = total_recon = total_kl = total_txt = 0.0\n",
        "        n_samples = 0\n",
        "        t0 = time.time()\n",
        "\n",
        "        for imgs, captions, lengths, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            img_recon, mu, logvar, logits, captions_tgt = model(\n",
        "                imgs, captions, lengths\n",
        "            )\n",
        "\n",
        "            loss, recon, kl, txt = multimodal_loss(\n",
        "                imgs, img_recon, mu, logvar, logits, captions_tgt, lengths\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_size = imgs.size(0)\n",
        "            n_samples += batch_size\n",
        "            total_loss += loss.item()\n",
        "            total_recon += recon.item()\n",
        "            total_kl += kl.item()\n",
        "            total_txt += txt.item()\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{epochs} | \"\n",
        "            f\"Loss: {total_loss/n_samples:.4f} | \"\n",
        "            f\"Recon: {total_recon/n_samples:.4f} | \"\n",
        "            f\"KL: {total_kl/n_samples:.4f} | \"\n",
        "            f\"Text: {total_txt/n_samples:.4f} | \"\n",
        "            f\"Time: {dt:.1f}s\"\n",
        "        )\n",
        "\n",
        "\n",
        "train(model, train_loader, optimizer, device, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e__udc2eqZvY",
        "outputId": "0b23930c-ca6b-4aed-cd57-378680acf442"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Loss: 118.7941 | Recon: 117.4002 | KL: 588.6555 | Text: 0.8053 | Time: 11.3s\n",
            "Epoch 2/5 | Loss: 63.7682 | Recon: 63.1771 | KL: 482.9101 | Text: 0.1082 | Time: 10.8s\n",
            "Epoch 3/5 | Loss: 58.9570 | Recon: 58.5184 | KL: 385.9859 | Text: 0.0527 | Time: 10.7s\n",
            "Epoch 4/5 | Loss: 56.8739 | Recon: 56.5022 | KL: 343.7999 | Text: 0.0279 | Time: 10.9s\n",
            "Epoch 5/5 | Loss: 55.6377 | Recon: 55.2999 | KL: 321.5235 | Text: 0.0163 | Time: 10.8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_caption_from_latent(\n",
        "    model: GVAMTLite,\n",
        "    fused_z: torch.Tensor,\n",
        "    max_len: int = 5,\n",
        ") -> str:\n",
        "    \"\"\"Greedy decoding of a caption from a fused latent vector.\n",
        "\n",
        "    Starts from the <sos> token and iteratively selects the highest-probability\n",
        "    token at each time step until <eos> is produced or max_len is reached.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        B = fused_z.size(0)\n",
        "        hidden = fused_z.unsqueeze(0)  # (1,B,H)\n",
        "\n",
        "        input_token = torch.full(\n",
        "            (B, 1), SOS_IDX, dtype=torch.long, device=device\n",
        "        )\n",
        "        generated = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            emb = model.txt_decoder.embed(input_token)\n",
        "            output, hidden = model.txt_decoder.gru(emb, hidden)\n",
        "            logits = model.txt_decoder.fc_out(output)  # (B,1,V)\n",
        "            token = logits.argmax(dim=-1)              # (B,1)\n",
        "            generated.append(token)\n",
        "            input_token = token\n",
        "\n",
        "        generated = torch.cat(generated, dim=1)  # (B, max_len)\n",
        "\n",
        "    tokens = generated[0].cpu().tolist()\n",
        "    words = []\n",
        "    for t in tokens:\n",
        "        if t == EOS_IDX:\n",
        "            break\n",
        "        if t in idx2word and t not in (PAD_IDX, SOS_IDX, EOS_IDX):\n",
        "            words.append(idx2word[t])\n",
        "    return \" \".join(words) if words else \"<empty>\"\n",
        "\n",
        "\n",
        "def show_samples(\n",
        "    model: GVAMTLite,\n",
        "    loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    n: int = 6,\n",
        ") -> None:\n",
        "    \"\"\"Visualizes original vs reconstructed images and prints a sample caption.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : GVAMTLite\n",
        "        Trained model.\n",
        "    loader : DataLoader\n",
        "        Dataloader (typically the test split).\n",
        "    device : torch.device\n",
        "        Execution device.\n",
        "    n : int, optional\n",
        "        Number of image pairs to show.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    imgs, captions, lengths, labels = next(iter(loader))\n",
        "    imgs = imgs.to(device)\n",
        "    captions = captions.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mu, logvar = model.img_encoder(imgs)\n",
        "        z_img = model.reparameterize(mu, logvar)\n",
        "        z_txt = model.txt_encoder(captions, lengths)\n",
        "        fused_input = torch.cat([z_img, z_txt], dim=1)\n",
        "        fused_z = model.fusion(fused_input)\n",
        "        img_recon = model.img_decoder(fused_z)\n",
        "\n",
        "    imgs_np = imgs.cpu().numpy()\n",
        "    recon_np = img_recon.cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(2 * n, 4))\n",
        "    for i in range(n):\n",
        "        # original\n",
        "        plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(imgs_np[i, 0], cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "        if i == 0:\n",
        "            plt.title(\"Original\")\n",
        "\n",
        "        # reconstructed\n",
        "        plt.subplot(2, n, n + i + 1)\n",
        "        plt.imshow(recon_np[i, 0], cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "        if i == 0:\n",
        "            plt.title(\"Reconstructed\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    decoded = decode_caption_from_latent(model, fused_z[:1], max_len=3)\n",
        "    true_word = digit_words[int(labels[0].item())]\n",
        "    print(\n",
        "        f\"Label: {labels[0].item()} | True text: {true_word} | \"\n",
        "        f\"Generated text: {decoded}\"\n",
        "    )\n",
        "\n",
        "\n",
        "show_samples(model, test_loader, device, n=6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "aqZM2dhjqbj0",
        "outputId": "99771337-53b6-4f10-c0a1-f654536b6fa6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 7 | True text: seven | Generated text: seven\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OjYU54M1qcYX"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}